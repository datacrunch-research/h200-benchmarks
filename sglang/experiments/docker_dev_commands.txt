# Docker env
# ref docker/compose.yaml https://github.com/sgl-project/sglang/blob/2b340adfb1ebf6dee420885479ee92296694078c/docker/compose.yaml#L22 
docker pull lmsysorg/sglang:dev
docker run -it -d --shm-size 32g --ulimit memlock=-1 --ulimit stack=67108864 --gpus all --net host \
--env "HF_TOKEN=$HF_TOKEN" \
-v /mnt/co-research/shared-models:/root/.cache/huggingface \
--ipc=host --name sglang_dev lmsysorg/sglang:latest bash

docker exec -it /bin/bash sglang_dev

# Benchmarking
# Llama 3.3 70B on 4 x H200 (bf16)
python3 -m sglang.launch_server --model-path meta-llama/Llama-3.3-70B-Instruct --tp-size 4 --host 0.0.0.0 --port 30000 --disable-radix-cache --enable-torch-compile

# LLaMA 405B on 8 X H200 (bf16)
python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-405B-Instruct --tp-size 8 --host 0.0.0.0 --port 30000 --disable-radix-cache --enable-torch-compile  --mem-frac 0.87

# DeepSeek-V2 on 8 x H200 (bf16)
python3 -m sglang.launch_server --model-path neuralmagic/DeepSeek-Coder-V2-Instruct-FP8 --disable-radix-cache --trust-remote-code --tp 8 --enable-dp-attention --mem-fraction-static 0.78
python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V2.5-1210 --disable-radix-cache --trust-remote-code --tp 8 --enable-dp-attention --mem-fraction-static 0.78 --enable-torch-compile

python3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input 1 --random-output 512 --random-range-ratio 1 --num-prompts 10000